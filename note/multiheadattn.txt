

nn.MultiheadAttention(d_model, n_head, dropout=dropout, batch_first=True)
    self.embed_dim
    self.kdim
    self.vdim
    self._qkv_same_embed_dim = True
    self.num_heads = num_heads
    self.dropout = dropout
    self.batch_first = batch_first (True)
    self.head_dim = embed_dim // num_heads

    self.in_proj_weight = Parameter(
        torch.empty((3 * embed_dim, embed_dim), **factory_kwargs)
    )

    self.in_proj_bias
    self.out_proj = NonDynamicallyQuantizableLinear(
        embed_dim, embed_dim, bias=bias, **factory_kwargs
    )
